Title: Make distributed cast test compatible with pytest 8

Summary:
- use allow_module_level=True for the GPU-count skip in
  tests/pytorch/distributed/test_cast_master_weights_to_fp8.py so pytest 8
  treats the module-level guard correctly

Testing:
- source venv/bin/activate && python -m pytest \
  tests/pytorch/distributed/test_cast_master_weights_to_fp8.py -k mxfp8 --maxfail=1
